# SAM-CLIP



# SAM + CLIP Open-Vocabulary Segmentation

This repository implements an open-vocabulary segmentation pipeline combining
Meta AI's Segment Anything Model (SAM) with OpenAI's CLIP.

The system generates dense candidate masks using SAM and filters them using
CLIP based on a natural language prompt.

---

## ğŸ” Pipeline Overview

1. **SAM (TensorFlow)** generates multiple candidate masks using grid-based point prompting.
2. **Mask post-processing** resizes masks to original image resolution.
3. **CLIP (PyTorch)** evaluates each masked region against a text prompt.
4. Masks exceeding a similarity threshold are retained and visualized.

---

## ğŸ§  Model Stack

| Model | Framework |
|-----|----------|
| SAM (ViT-L) | TensorFlow |
| CLIP (ViT-B/32) | PyTorch |

---

Results example: prompt given = "small blocks or microstructures" (The segmented object will be masked in red colour)
<img width="542" height="395" alt="image" src="https://github.com/user-attachments/assets/f633ebd7-29ca-47c8-aa78-20e9db6ea62f" />

---

## ğŸ“ Project Structure

```text
sam-clip-open-vocabulary-segmentation/
â”œâ”€â”€ sam_clip/
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ sam_inference.py
â”‚   â”œâ”€â”€ clip_filtering.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_single_image.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ demo.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
